{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 2.x\n",
    "# How to use pre-trained embeddings:\n",
    "# https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense, Flatten, LSTM, Input, Embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing training, valid & test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = []\n",
    "valid_ids = []\n",
    "test_ids = []\n",
    "\n",
    "train_class = []\n",
    "valid_class = []\n",
    "\n",
    "train_texts = []\n",
    "valid_texts = []\n",
    "test_texts = []\n",
    "\n",
    "train = open('/home/kunst/utterances.train', 'r')\n",
    "for line in train:\n",
    "    split = line.split(\"\\t\")\n",
    "    # save ids\n",
    "    train_ids.append(split[0])\n",
    "    # save classifications\n",
    "    train_class.append(split[1])\n",
    "    data = split[2].split(\";\")\n",
    "    data[-1] = data[-1].strip() # deleting \\n\n",
    "    # save uts\n",
    "    train_texts.append(data)\n",
    "train.close()\n",
    "train_texts = np.array(train_texts)\n",
    "\n",
    "valid = open('/home/kunst/utterances.valid', 'r')\n",
    "for line in valid:\n",
    "    split = line.split(\"\\t\")\n",
    "    valid_ids.append(split[0])\n",
    "    valid_class.append(split[1])\n",
    "    data = split[2].split(\";\")\n",
    "    data[-1] = data[-1].strip() # deleting \\n\n",
    "    valid_texts.append(data)\n",
    "valid.close()\n",
    "valid_texts = np.array(valid_texts)\n",
    "\n",
    "test = open('/home/kunst/utterances.test', 'r')\n",
    "for line in test:\n",
    "    split = line.split(\"\\t\")\n",
    "    test_ids.append(split[0])\n",
    "    data = split[1].split(\";\")\n",
    "    data[-1] = data[-1].strip() # deleting \\n\n",
    "    test_texts.append(data)\n",
    "test.close()\n",
    "test_texts = np.array(test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification = np.unique(train_class)\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(train_class)\n",
    "y_valid = lb.fit_transform(valid_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORD_COUNT = 1000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORD_COUNT)\n",
    "tokenizer.fit_on_texts(train_texts.flatten().tolist()+valid_texts.flatten().tolist())\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "# training data with all four uts\n",
    "x_train =[]\n",
    "for ar in train_texts:\n",
    "    sequences = tokenizer.texts_to_sequences(ar)\n",
    "    x_train.append(pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH))\n",
    "x_train = np.array(x_train)\n",
    "x_valid =[]\n",
    "for ar in valid_texts:\n",
    "    sequences = tokenizer.texts_to_sequences(ar)\n",
    "    x_valid.append(pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH))\n",
    "x_valid = np.array(x_valid)\n",
    "x_test =[]\n",
    "for ar in test_texts:\n",
    "    sequences = tokenizer.texts_to_sequences(ar)\n",
    "    x_test.append(pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH))\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "# training data with only last ut\n",
    "x_train_2 = []\n",
    "for ar in x_train:\n",
    "    x_train_2.append(ar[3])\n",
    "x_train_2 = np.array(x_train_2)\n",
    "x_valid_2 = []\n",
    "for ar in x_valid:\n",
    "    x_valid_2.append(ar[3])\n",
    "x_valid_2 = np.array(x_valid_2)\n",
    "x_test_2 = []\n",
    "for ar in x_test:\n",
    "    x_test_2.append(ar[3])\n",
    "x_test_2 = np.array(x_test_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute an index mapping words to known embeddings, by parsing the data dump of pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "w2v = open('word2vec_embeddings.txt', 'r')\n",
    "for line in w2v:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "w2v.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        # words not found in embedding index will assigned random vector\n",
    "        embedding_matrix[i] = np.random.uniform(-0.25, 0.25, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 100, 300)          7041300   \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 30000)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 200)               6000200   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 31)                6231      \n",
      "=================================================================\n",
      "Total params: 13,168,331\n",
      "Trainable params: 13,168,331\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "modelDense = Sequential()\n",
    "modelDense.add(Embedding(len(word_index) + 1,EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, weights=[embedding_matrix]))\n",
    "modelDense.add(Flatten())\n",
    "modelDense.add(Dense(units=200, kernel_initializer='random_uniform', activation='sigmoid'))\n",
    "modelDense.add(Dropout(0.3))\n",
    "modelDense.add(Dense(units=200, kernel_initializer='random_uniform', activation='relu'))\n",
    "modelDense.add(Dropout(0.2))\n",
    "modelDense.add(Dense(units=200, kernel_initializer='random_uniform', activation='sigmoid'))\n",
    "modelDense.add(Dropout(0.1))\n",
    "modelDense.add(Dense(units=200, kernel_initializer='random_uniform', activation='relu'))\n",
    "modelDense.add(Dense(units=len(classification), kernel_initializer='random_uniform', activation='softmax'))\n",
    "modelDense.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "modelDense.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 100, 300)          7041300   \n",
      "_________________________________________________________________\n",
      "simple_rnn_9 (SimpleRNN)     (None, 100, 100)          40100     \n",
      "_________________________________________________________________\n",
      "simple_rnn_10 (SimpleRNN)    (None, 100, 100)          20100     \n",
      "_________________________________________________________________\n",
      "simple_rnn_11 (SimpleRNN)    (None, 100, 100)          20100     \n",
      "_________________________________________________________________\n",
      "simple_rnn_12 (SimpleRNN)    (None, 100, 100)          20100     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100, 100)          10100     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 100)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 100)          0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 31)                310031    \n",
      "=================================================================\n",
      "Total params: 7,461,831\n",
      "Trainable params: 420,531\n",
      "Non-trainable params: 7,041,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import SimpleRNN, Dropout\n",
    "modelRNN = Sequential()\n",
    "modelRNN.add(Embedding(len(word_index) + 1,EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, weights=[embedding_matrix], trainable=False))\n",
    "modelRNN.add(SimpleRNN(units=100, return_sequences=True, activation='relu'))\n",
    "modelRNN.add(SimpleRNN(units=100, return_sequences=True, activation='relu'))\n",
    "modelRNN.add(SimpleRNN(units=100, return_sequences=True, activation='relu'))\n",
    "modelRNN.add(SimpleRNN(units=100, return_sequences=True, activation='relu'))\n",
    "modelRNN.add(Dense(units=100, kernel_initializer='random_uniform', activation='relu'))\n",
    "modelRNN.add(Dropout(0.2))\n",
    "modelDense.add(Dense(units=100, kernel_initializer='random_uniform', activation='relu'))\n",
    "modelRNN.add(Dropout(0.2))\n",
    "modelRNN.add(Flatten())\n",
    "modelRNN.add(Dense(units=31, kernel_initializer='random_uniform', activation='softmax'))\n",
    "modelRNN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])#\n",
    "modelRNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM - RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 100, 300)          7041300   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 31)                3131      \n",
      "=================================================================\n",
      "Total params: 7,204,831\n",
      "Trainable params: 163,531\n",
      "Non-trainable params: 7,041,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Input\n",
    "\n",
    "# Input layer\n",
    "input_x = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='float32')\n",
    "\n",
    "# Embedding layer\n",
    "embeddings = Embedding(len(word_index) + 1,EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, weights=[embedding_matrix])(input_x)\n",
    "\n",
    "# Reshape to 3D\n",
    "lstm = LSTM(units=100, activation='tanh', )(embeddings)\n",
    "\n",
    "dense0 = Dense(units=100, kernel_initializer='random_uniform', activation='relu')(lstm)\n",
    "\n",
    "# Softmax\n",
    "dense = Dense(units=len(classification), kernel_initializer='random_uniform', activation='softmax')(dense0)\n",
    "\n",
    "# Defining model\n",
    "modelLSTM = Model(input_x, dense)\n",
    "\n",
    "# Model compilation, Adam optimizer\n",
    "modelLSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "modelLSTM.summary()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 4, 100)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 4, 100, 300)  7041300     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 3, 99, 100)   120100      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 1, 97, 100)   480100      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 99, 100)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 97, 100)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 9900)         0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 9700)         0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 19600)        0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          1960100     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 31)           3131        dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,604,731\n",
      "Trainable params: 9,604,731\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Conv2D, MaxPooling2D, Flatten, Reshape\n",
    "from keras.layers.merge import Concatenate\n",
    "\n",
    "# Input layer\n",
    "input_x = Input(shape=(4,MAX_SEQUENCE_LENGTH), dtype='float32')\n",
    "\n",
    "# Embedding layer\n",
    "embeddings = Embedding(len(word_index) + 1,EMBEDDING_DIM, input_shape=(4,MAX_SEQUENCE_LENGTH), weights=[embedding_matrix])(input_x)\n",
    "\n",
    "num_filters = 100\n",
    "\n",
    "pooled_outputs = []\n",
    "conv1 = Conv2D(num_filters, kernel_size=2, activation='tanh')(embeddings)\n",
    "pooling1 = MaxPooling2D(pool_size=(int(conv1.shape[1]), 1))(conv1)\n",
    "pooled_flat1 = Flatten()(pooling1)\n",
    "pooled_outputs.append(pooled_flat1)\n",
    "conv2 = Conv2D(num_filters, kernel_size=4, activation='tanh')(embeddings)\n",
    "pooling2 = MaxPooling2D(pool_size=(int(conv2.shape[1]), 1))(conv2)\n",
    "pooled_flat2 = Flatten()(pooling2)\n",
    "pooled_outputs.append(pooled_flat2)\n",
    "    \n",
    "# Concatenation of maxpooling outputs\n",
    "h_pool = Concatenate()(pooled_outputs)\n",
    "\n",
    "# Dense\n",
    "dense = Dense(units=100, kernel_initializer='random_uniform', activation='relu')(h_pool)\n",
    "\n",
    "# Softmax\n",
    "output = Dense(len(classification), activation='softmax')(dense)\n",
    "\n",
    "# Model\n",
    "modelCNN = Model(input_x, output)\n",
    "\n",
    "# Model compilation, Adam optimizer\n",
    "modelCNN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "modelCNN.summary()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 196502 samples, validate on 20000 samples\n",
      "Epoch 1/5\n",
      "196502/196502 [==============================] - 127s 647us/step - loss: 1.7619 - acc: 0.5073 - val_loss: 2.1536 - val_acc: 0.3987\n",
      "Epoch 2/5\n",
      "196502/196502 [==============================] - 125s 636us/step - loss: 1.4427 - acc: 0.6291 - val_loss: 1.8588 - val_acc: 0.5831\n",
      "Epoch 3/5\n",
      "196502/196502 [==============================] - 136s 693us/step - loss: 1.2231 - acc: 0.6787 - val_loss: 1.9049 - val_acc: 0.5735\n",
      "Epoch 4/5\n",
      "196502/196502 [==============================] - 136s 692us/step - loss: 1.1369 - acc: 0.6900 - val_loss: 1.9360 - val_acc: 0.4889\n",
      "Epoch 5/5\n",
      "196502/196502 [==============================] - 134s 683us/step - loss: 1.0709 - acc: 0.6972 - val_loss: 1.9419 - val_acc: 0.4861\n",
      "Accuracy: 48.61%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = modelDense.fit(x_train_2, y_train, validation_data=(x_valid_2, y_valid), epochs=5, batch_size=4000)\n",
    "# Final evaluation of the model\n",
    "scores = modelDense.evaluate(x_valid_2, y_valid, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "196502/196502 [==============================] - 467s 2ms/step - loss: 1.7097 - acc: 0.5870\n",
      "Epoch 2/5\n",
      "196502/196502 [==============================] - 465s 2ms/step - loss: 1.2358 - acc: 0.6791\n",
      "Epoch 3/5\n",
      "196502/196502 [==============================] - 466s 2ms/step - loss: 1.0903 - acc: 0.7040\n",
      "Epoch 4/5\n",
      "196502/196502 [==============================] - 482s 2ms/step - loss: 0.9951 - acc: 0.7225\n",
      "Epoch 5/5\n",
      "196502/196502 [==============================] - 471s 2ms/step - loss: 0.9083 - acc: 0.7405\n",
      "Accuracy: 53.11%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "modelRNN.fit(x_train_2, y_train, epochs=5, batch_size=4000)\n",
    "# Final evaluation of the model\n",
    "scores = modelRNN.evaluate(x_valid_2, y_valid, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "196502/196502 [==============================] - 389s 2ms/step - loss: 2.2211 - acc: 0.5088\n",
      "Epoch 2/5\n",
      "196502/196502 [==============================] - 385s 2ms/step - loss: 1.5505 - acc: 0.6120\n",
      "Epoch 3/5\n",
      "196502/196502 [==============================] - 387s 2ms/step - loss: 1.3923 - acc: 0.6432\n",
      "Epoch 4/5\n",
      "196502/196502 [==============================] - 385s 2ms/step - loss: 1.2688 - acc: 0.6686\n",
      "Epoch 5/5\n",
      "196502/196502 [==============================] - 387s 2ms/step - loss: 1.1630 - acc: 0.6964\n",
      "Accuracy: 57.02%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "modelLSTM.fit(x_train_2, y_train, epochs=5, batch_size=4000)\n",
    "# Final evaluation of the model\n",
    "scores = modelLSTM.evaluate(x_valid_2, y_valid, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 196502 samples, validate on 20000 samples\n",
      "Epoch 1/3\n",
      "196502/196502 [==============================] - 2308s 12ms/step - loss: 1.0853 - acc: 0.7059 - val_loss: 1.6908 - val_acc: 0.5863\n",
      "Epoch 2/3\n",
      "196502/196502 [==============================] - 2584s 13ms/step - loss: 0.7363 - acc: 0.7842 - val_loss: 1.7213 - val_acc: 0.5769\n",
      "Epoch 3/3\n",
      "196502/196502 [==============================] - 2527s 13ms/step - loss: 0.6396 - acc: 0.8068 - val_loss: 1.7849 - val_acc: 0.5853\n",
      "Accuracy: 58.53%\n"
     ]
    }
   ],
   "source": [
    "history = modelCNN.fit(x_train, y_train, validation_data=(x_valid, y_valid), epochs=3, batch_size=1024)\n",
    "scores = modelCNN.evaluate(x_valid, y_valid, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = modelCNN.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testfile schreiben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "max_pos = np.argmax(prediction, axis=1)\n",
    "for i in range (0, max_pos.shape[0]):\n",
    "    labels.append(classification[max_pos[i]])\n",
    "labels = np.array(labels)\n",
    "\n",
    "filename = \"2693266_Hoshaber_Topic1_result.txt\"\n",
    "file = open(filename, 'w')\n",
    "for i in range (0,np.array(test_ids).shape[0]):\n",
    "    text = str(np.array(test_ids)[i]) + \"\\t\" + str(labels[i] + \"\\n\")\n",
    "    file.write(text)\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
